# -*- coding: utf-8 -*-
"""lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12WTeF5VtpNULn2ci-zYX9aTpWHYePf3r

### Import packages
"""

import pandas as pd;
import matplotlib.pyplot as plt;
import numpy as np;

"""### Load data"""

train = pd.read_csv('train.csv')
valid = pd.read_csv('valid.csv')

"""### Describe data"""

train.shape

train.head()

train.describe(include='all')

"""### Handle missing values

"""

columns = train.columns.values
labels = columns[-4:]
features = columns[:-4]

train[labels].isnull().sum()

valid[labels].isnull().sum()

from sklearn.impute import SimpleImputer

# missing values are imputed with the median value
imp = SimpleImputer(missing_values=np.nan, strategy='median')
imp.fit(train[labels])
imp.fit(valid[labels])
train[labels] = imp.transform(train[labels])
valid[labels] =imp.transform(valid[labels])

train[labels].isnull().sum()

valid[labels].isnull().sum()

X_train = train[features]
Y_train = train[labels]
X_valid = valid[features]
Y_valid = valid[labels]

"""### Removing features with low variance"""

from sklearn.feature_selection import VarianceThreshold
threshold = VarianceThreshold(threshold=1)
threshold.fit_transform(X_train)
threshold.fit_transform(X_valid)
X_train.shape, X_valid.shape

"""##### no columns were removed

### Check for Correlations between features
##### if there are strongly correlated features using just one of them should be enough
"""

import seaborn as sns
#Using Pearson Correlation
X_train.corr()

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train, 0.7)
len(set(corr_features))

"""##### There are no features that exceed the correlation threshold of 0.7

## Label 1 - Speaker ID
"""

Y_train_1 = Y_train['label_1']
Y_valid_1 = Y_valid['label_1']

Y_train_1.head()

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

poly_1_original = SVC(kernel='poly', degree=3, C=1).fit(X_train, Y_train_1)
poly_pred = poly_1_original.predict(X_valid)
poly_accuracy = accuracy_score(Y_valid_1, poly_pred)
print('Original accuracy: ', "%.2f" % (poly_accuracy*100))

"""### Mutual Information"""

from sklearn.feature_selection import mutual_info_classif

mutual_info = mutual_info_classif(X_train, Y_train_1)
mutual_info = pd.Series(mutual_info)
mutual_info.index = X_train.columns
mutual_info.sort_values(ascending=False)

mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))

"""#### Lets remove features with information gain less than 0.2 and calculate the acuuracy again"""

from sklearn.feature_selection import SelectKBest

# selecting features with greater than 0.25 mutual information scores
X_train_1 = X_train.copy()
X_valid_1 = X_valid.copy()
feature_count_1 = mutual_info[mutual_info > 0.19].count()
sel_five_cols = SelectKBest(mutual_info_classif, k=feature_count_1)
sel_five_cols.fit(X_train_1, Y_train_1)
new_columns_1 = X_train_1.columns[sel_five_cols.get_support()]

print(new_columns_1)

print(f'{feature_count_1} features selected out of 260')

X_train_1 = X_train_1[new_columns_1]
X_valid_1 = X_valid_1[new_columns_1]
X_train_1.shape, X_valid_1.shape

poly_1_new = SVC(kernel='poly', degree=3, C=1).fit(X_train_1, Y_train_1)
poly_pred = poly_1_new.predict(X_valid_1)
poly_accuracy = accuracy_score(Y_valid_1, poly_pred)
print('New accuracy: ', "%.2f" % (poly_accuracy*100))

"""## Label 2 - Speaker Age

### Apply Support Vector Machine to the original dataset
"""

Y_train_2 = Y_train['label_2']
Y_valid_2 = Y_valid['label_2']

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

poly_2_original = SVC(kernel='poly', degree=3, C=1).fit(X_train, Y_train_2)
poly_pred = poly_2_original.predict(X_valid)
poly_accuracy = accuracy_score(Y_valid_2, poly_pred)
print('Original accuracy: ', "%.2f" % (poly_accuracy*100))

"""### Mutual Information"""

from sklearn.feature_selection import mutual_info_classif

mutual_info = mutual_info_classif(X_train, Y_train_2)
mutual_info = pd.Series(mutual_info)
mutual_info.index = X_train.columns
mutual_info.sort_values(ascending=False)

mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))

"""#### Lets remove features with information gain less than 0.2 and calculate the acuuracy again"""

from sklearn.feature_selection import SelectKBest

# selecting features with greater than 0.25 mutual information scores
X_train_2 = X_train.copy()
X_valid_2 = X_valid.copy()
feature_count_2 = mutual_info[mutual_info > 0.055].count()
sel_five_cols_2 = SelectKBest(mutual_info_classif, k=feature_count_2)
sel_five_cols_2.fit(X_train_2, Y_train_2)
new_columns_2 = X_train_2.columns[sel_five_cols_2.get_support()]

print(new_columns_2)

print(f'{feature_count_2} features selected out of 260')

X_train_2 = X_train_2[new_columns_2]
X_valid_2 = X_valid_2[new_columns_2]
X_train_2.shape, X_valid_2.shape

poly_2_new = SVC(kernel='poly', degree=3, C=1).fit(X_train_2, Y_train_2)
poly_pred = poly_2_new.predict(X_valid_2)
poly_accuracy = accuracy_score(Y_valid_2, poly_pred)
print('New accuracy: ', "%.2f" % (poly_accuracy*100))

"""## Label 3 - Speaker Gender

### Apply Support Vector Machine to the original dataset
"""

Y_train_3 = Y_train['label_3']
Y_valid_3 = Y_valid['label_3']

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

poly_3_original = SVC(kernel='poly', degree=3, C=1).fit(X_train, Y_train_3)
poly_pred = poly_3_original.predict(X_valid)
poly_accuracy = accuracy_score(Y_valid_3, poly_pred)
print('Original accuracy: ', "%.2f" % (poly_accuracy*100))

"""### Mutual Information"""

from sklearn.feature_selection import mutual_info_classif

mutual_info = mutual_info_classif(X_train, Y_train_3)
mutual_info = pd.Series(mutual_info)
mutual_info.index = X_train.columns
mutual_info.sort_values(ascending=False)

mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))

"""#### Lets remove features with information gain less than 0.2 and calculate the acuuracy again"""

from sklearn.feature_selection import SelectKBest

# selecting features with greater than 0.25 mutual information scores
X_train_3 = X_train.copy()
X_valid_3 = X_valid.copy()
feature_count_3 = mutual_info[mutual_info > 0.05].count()
sel_five_cols_3 = SelectKBest(mutual_info_classif, k=feature_count_3)
sel_five_cols_3.fit(X_train_3, Y_train_3)
new_columns_3 = X_train_3.columns[sel_five_cols_3.get_support()]

print(new_columns_3)

print(f'{feature_count_3} features selected out of 260')

X_train_3 = X_train_3[new_columns_3]
X_valid_3 = X_valid_3[new_columns_3]
X_train_3.shape, X_valid_3.shape

poly_3_new = SVC(kernel='poly', degree=3, C=1).fit(X_train_3, Y_train_3)
poly_pred = poly_3_new.predict(X_valid_3)
poly_accuracy = accuracy_score(Y_valid_3, poly_pred)
print('New accuracy: ', "%.2f" % (poly_accuracy*100))

"""## Label 4 - Speaker Accent

### Apply Support Vector Machine to the original dataset
"""

Y_train_4 = Y_train['label_4']
Y_valid_4 = Y_valid['label_4']

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

poly_4_original = SVC(kernel='poly', class_weight='balanced', degree=3, C=1).fit(X_train, Y_train_4)
poly_pred = poly_4_original.predict(X_valid)
poly_accuracy = accuracy_score(Y_valid_4, poly_pred)
print('Original accuracy: ', "%.2f" % (poly_accuracy*100))

"""### Mutual Information"""

from sklearn.feature_selection import mutual_info_classif

mutual_info = mutual_info_classif(X_train, Y_train_4)
mutual_info = pd.Series(mutual_info)
mutual_info.index = X_train.columns
mutual_info.sort_values(ascending=False)

mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))

"""#### Lets remove features with information gain less than 0.2 and calculate the acuuracy again"""

from sklearn.feature_selection import SelectKBest

# selecting features with greater than 0.25 mutual information scores
X_train_4 = X_train.copy()
X_valid_4 = X_valid.copy()
feature_count_4 = mutual_info[mutual_info > 0.035].count()
sel_five_cols_4 = SelectKBest(mutual_info_classif, k=feature_count_4)
sel_five_cols_4.fit(X_train_4, Y_train_4)
new_columns_4 = X_train_4.columns[sel_five_cols_4.get_support()]

print(new_columns_4)

print(f'{feature_count_4} features selected out of 260')

X_train_4 = X_train_4[new_columns_4]
X_valid_4 = X_valid_4[new_columns_4]
X_train_4.shape, X_valid_4.shape

poly_4_new = SVC(kernel='poly', class_weight= 'balanced', degree=3, C=1).fit(X_train_4, Y_train_4)
poly_pred = poly_4_new.predict(X_valid_4)
poly_accuracy = accuracy_score(Y_valid_4, poly_pred)
print('New accuracy: ', "%.2f" % (poly_accuracy*100))

"""## Testing"""

test = pd.read_csv('test.csv')

X_test = test[features]
X_test_1 = test[new_columns_1]
X_test_2 = test[new_columns_2]
X_test_3 = test[new_columns_3]
X_test_4 = test[new_columns_4]

def predict(X_test, new_columns, feature_count, model_org, model_new, filename):
    df = pd.DataFrame()
    X_test_new = X_test[new_columns]
    test_org = model_org.predict(X_test)
    df['Predicted labels before feature engineering'] = test_org
    test_new = model_new.predict(X_test_new)
    df['Predicted labels after feature engineering'] = test_new
    df['No of new features'] = feature_count
    for feature in features:
        new_feature = 'new_' + feature
        if feature not in new_columns:
          df[new_feature] = pd.NA
        else:
          df[new_feature] = X_test[feature]
    df.to_csv(filename, index=False)

predict(X_test, new_columns_1, feature_count_1, poly_1_original, poly_1_new, '190159D_label_1.csv')
predict(X_test, new_columns_2, feature_count_2, poly_2_original, poly_2_new, '190159D_label_2.csv')
predict(X_test, new_columns_3, feature_count_3, poly_3_original, poly_3_new, '190159D_label_3.csv')
predict(X_test, new_columns_4, feature_count_4, poly_4_original, poly_4_new, '190159D_label_4.csv')